{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmistroni/OpenBB_Experiments/blob/main/openbbPlatformAsLLMTools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86fD0l2_HQnE"
      },
      "source": [
        "# OpenBB Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIXVduLoHQnG"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lgx0Gz7jHQnG"
      },
      "source": [
        "### Install dependencies\n",
        "####https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/language/orchestration/langchain/intro_langchain_palm_api.ipynb#scrollTo=eVpPcvsrkzCk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJPCzdV-HQnH"
      },
      "outputs": [],
      "source": [
        "!pip install openbb\n",
        "!pip install openbb-charting\n",
        "\n",
        "!pip install pydantic\n",
        "\n",
        "!pip install langchain\n",
        "!pip install langchain_community\n",
        "#!pip install langchain_openai\n",
        "!pip install langchain-core\n",
        "!pip install langchain-google-vertexai\n",
        "!pip install -U langchain-google-genai\n",
        "# need to restart runtime after installing pydantic"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set the VertexAI Key"
      ],
      "metadata": {
        "id": "3kLWP09WJhDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.api_core import retry\n",
        "from google.colab import userdata\n",
        "gkey = userdata.get('AI_STUDIO_KEY')\n",
        "pat = userdata.get('PAT')\n",
        "genai.configure(api_key=gkey)\n",
        "import vertexai\n",
        "from google.colab import auth\n",
        "from vertexai.generative_models import (\n",
        "    Content,\n",
        "    FunctionDeclaration,\n",
        "    GenerationConfig,\n",
        "    GenerativeModel,\n",
        "    Part,\n",
        "    Tool,\n",
        "    ToolConfig\n",
        ")\n",
        "PROJECT_ID = \"datascience-projects\"  # @param {type:\"string\"}\n",
        "# Set the project id\n",
        "#! gcloud config set project {PROJECT_ID}\n",
        "#auth.authenticate_user()\n",
        "vertexai.init(\n",
        "    project=PROJECT_ID,\n",
        "    location=\"us-central1\",\n",
        "    staging_bucket=\"gs://mm_dataflow_bucket\",\n",
        ")\n",
        "model = GenerativeModel(model_name=\"gemini-1.5-pro-001\")"
      ],
      "metadata": {
        "id": "R6Z4svaOJjvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Vertex AI Custom function api"
      ],
      "metadata": {
        "id": "7ZNinTX7OvC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "function_name = \"get_current_weather\"\n",
        "get_current_weather_func = FunctionDeclaration(\n",
        "    name=function_name,\n",
        "    description=\"Get the current weather in a given location\",\n",
        "    parameters={\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The city name of the location for which to get the weather.\"}},\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "F-V8SecSOy9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the user's prompt in a Content object that we can reuse in model calls\n",
        "user_prompt_content = Content(\n",
        "    role=\"user\",\n",
        "    parts=[\n",
        "        Part.from_text(\"What is the weather like in Boston?\"),\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "8OLwn4FiPY-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a tool that includes the function declaration get_current_weather_func\n",
        "weather_tool = Tool(\n",
        "    function_declarations=[get_current_weather_func],\n",
        ")\n",
        "\n",
        "# Send the prompt and instruct the model to generate content using the Tool object that you just created\n",
        "response = model.generate_content(\n",
        "    user_prompt_content,\n",
        "    generation_config={\"temperature\": 0},\n",
        "    tools=[weather_tool],\n",
        "    tool_config=ToolConfig(\n",
        "        function_calling_config=ToolConfig.FunctionCallingConfig(\n",
        "            # ANY mode forces the model to predict a function call\n",
        "            mode=ToolConfig.FunctionCallingConfig.Mode.ANY,\n",
        "            # Allowed functions to call when the mode is ANY. If empty, any one of\n",
        "            # the provided functions are called.\n",
        "            allowed_function_names=[\"get_current_weather\"],\n",
        "    ))\n",
        ")\n",
        "response_function_call_content = response.candidates[0].content\n",
        "print(response_function_call_content)"
      ],
      "metadata": {
        "id": "wlZzHWIIPQWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the function name that the model responded with, and make an API call to an external system\n",
        "if (\n",
        "    response.candidates[0].content.parts[0].function_call.name\n",
        "    == \"get_current_weather\"\n",
        "):\n",
        "    # Extract the arguments to use in your API call\n",
        "    location = (\n",
        "        response.candidates[0].content.parts[0].function_call.args[\"location\"]\n",
        "    )\n",
        "\n",
        "    # Here you can use your preferred method to make an API request to fetch the current weather, for example:\n",
        "    # api_response = requests.post(weather_api_url, data={\"location\": location})\n",
        "\n",
        "    # In this example, we'll use synthetic data to simulate a response payload from an external API\n",
        "    api_response = \"\"\"{ \"location\": \"Boston, MA\", \"temperature\": 38, \"description\": \"Partly Cloudy\",\n",
        "                    \"icon\": \"partly-cloudy\", \"humidity\": 65, \"wind\": { \"speed\": 10, \"direction\": \"NW\" } }\"\"\""
      ],
      "metadata": {
        "id": "H41AAbshriLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    [\n",
        "        user_prompt_content,  # User prompt\n",
        "        response_function_call_content,  # Function call response\n",
        "        Content(\n",
        "            parts=[\n",
        "                Part.from_function_response(\n",
        "                    name=\"get_current_weather\",\n",
        "                    response={\n",
        "                        \"content\": api_response,  # Return the API response to Gemini\n",
        "                    },\n",
        "                )\n",
        "            ],\n",
        "        ),\n",
        "    ],\n",
        "    tools=[weather_tool],\n",
        ")\n",
        "# Get the model summary response\n",
        "summary = response.candidates[0].content.parts[0].text\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "ywiigWxCrbk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKSfogJeHQnJ"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGkfPhGrHQnJ"
      },
      "outputs": [],
      "source": [
        "from openbb import obb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEdbozfgHQnJ"
      },
      "source": [
        "### Login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjqE5z5GHQnK"
      },
      "outputs": [],
      "source": [
        "obb.account.login(pat=pat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zrASz1YHQnK"
      },
      "source": [
        "## OpenBB tool pre-requisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECRWUpM2HQnK"
      },
      "source": [
        "### Callable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1azxmLAFHQnK"
      },
      "outputs": [],
      "source": [
        "obb.equity.price.historical(symbol=\"AAPL\", provider=\"fmp\").to_df()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4W6Dh8KHQnL"
      },
      "outputs": [],
      "source": [
        "obb.equity.price.historical(\"AAPL\", start_date=\"2022-01-01\", provider='fmp')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVg0I07zHQnL"
      },
      "source": [
        "### Input Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-EScCueHQnL"
      },
      "outputs": [],
      "source": [
        "obb.coverage.command_model[\".equity.price.historical\"][\"openbb\"][\"QueryParams\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7C-mWzhIHQnL"
      },
      "outputs": [],
      "source": [
        "obb.coverage.command_model[\".equity.price.historical\"][\"polygon\"][\"QueryParams\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kECpK203HQnL"
      },
      "source": [
        "### Documentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAzOuZHgHQnM"
      },
      "outputs": [],
      "source": [
        "help(obb.equity.price.historical)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvZPyjjsHQnM"
      },
      "source": [
        "## OpenBB tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qNxZQChHQnM"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tools import StructuredTool\n",
        "\n",
        "llm_historical_price = StructuredTool.from_function(\n",
        "    func=obb.equity.price.historical,\n",
        "    description=obb.equity.price.historical.__doc__.split('\\n')[0]  # Use first line of docstring\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ML2koKHHQnM"
      },
      "source": [
        "## Multiple OpenBB Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYs0KwiRHQnM"
      },
      "outputs": [],
      "source": [
        "llm_tools = [\n",
        "    StructuredTool.from_function(\n",
        "        name=name,\n",
        "        func=schema['callable'],\n",
        "        description=schema['callable'].__doc__.split('\\n')[0]\n",
        "    )\n",
        "    for name, schema\n",
        "    in obb.coverage.command_schemas().items()\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg1gYtIOHQnM"
      },
      "source": [
        "## OpenBB Tools fed to agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJo2TDyLHQnM"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import AgentExecutor, create_openai_functions_agent, create_tool_calling_agent\n",
        "from langchain_google_vertexai import VertexAI, ChatVertexAI #, ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a very powerful assistant, but don't know current events\"),\n",
        "        (\"user\", \"{input}\"),\n",
        "        MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "llm_tools = [\n",
        "    StructuredTool.from_function(\n",
        "        func=obb.equity.price.quote,\n",
        "        description=obb.equity.price.quote.__doc__.split(\"\\n\")[0]\n",
        "    )\n",
        "]\n",
        "\n",
        "#llm = VertexAI(model_name=\"gemini-pro\")\n",
        "llm = ChatVertexAI(model=\"gemini-pro\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "from langchain.llms import VertexAI\n",
        "from langchain.embeddings import VertexAIEmbeddings\n",
        "from langchain.chat_models import ChatVertexAI\n",
        "from google.cloud import aiplatform\n",
        "import time\n",
        "from typing import List\n",
        "\n",
        "# LangChain\n",
        "import langchain\n",
        "from pydantic import BaseModel\n",
        "\n",
        "print(f\"LangChain version: {langchain.__version__}\")\n",
        "\n",
        "# Vertex AI\n",
        "\n",
        "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")"
      ],
      "metadata": {
        "id": "wAhl_KzHaFwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = create_tool_calling_agent(llm=llm, tools=llm_tools, prompt=prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=llm_tools, verbose=True)\n",
        "\n",
        "agent_executor.invoke({\"input\": \"What is the latest stock price of AAPL?\"})"
      ],
      "metadata": {
        "id": "hMStzdykZoMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jnrGKBZuL7tc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}